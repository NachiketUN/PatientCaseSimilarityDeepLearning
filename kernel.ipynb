{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1db49d1c8e9167fda6ca9eace681634f5b080278"
   },
   "outputs": [],
   "source": [
    "pat_df=pd.read_csv('../input/PATIENTS.csv',low_memory=False)\n",
    "pat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "647160f4c5d751048e93ba4250fdb1af1d3a1875"
   },
   "outputs": [],
   "source": [
    "pres_df=pd.read_csv('../input/PRESCRIPTIONS.csv',low_memory=False)\n",
    "pres_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "note_df=pd.read_csv('../input/NOTEEVENTS.csv',low_memory=False)\n",
    "note_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a2a945d71ed6d006ed9102fba24fffde49de222"
   },
   "outputs": [],
   "source": [
    "# print(\"Total no. of rows in this csv:{}\".format(note_df.shape[0]))\n",
    "# print(\"Different types of category in this csv are:\\n{}\".format(note_df.CATEGORY.value_counts()))\n",
    "# print(\"Different types of description in this csv are:\\n{}\".format(note_df.DESCRIPTION.value_counts()))\n",
    "# print(\"Unique patient ids and their number of arrivals to the hospital are:\\n{}\".format(note_df.SUBJECT_ID.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "272393ae605b501f993296ad9ac1afc3c587653e"
   },
   "outputs": [],
   "source": [
    "print(\"Number of unique patients:{}\".format(len(note_df.SUBJECT_ID.unique())))\n",
    "print(\"Number of patients who have visited the hospital only once:{}\".format(note_df.shape[0]-len(note_df[note_df.duplicated(['SUBJECT_ID'],keep=False)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17ab70ede38c816edd55176e3d41ccf72173ca70"
   },
   "outputs": [],
   "source": [
    "#word counts\n",
    "note_df['word_count'] = note_df['TEXT'].apply(lambda x: len(str(x).split(\" \")))\n",
    "note_df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa1b696255a59a563b357087ed02635bb6fa016b"
   },
   "outputs": [],
   "source": [
    "train_des = note_df['TEXT'].fillna(\"not available\").values\n",
    "#train_subid=req_df['SUBJECT_ID'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a8b470b45abf51583f5386cf14d69770a783380"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "#lemmatizer = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def data_clean(big_list):\n",
    "    for i in range(len(big_list)):\n",
    "        curr_string=big_list[i]\n",
    "        curr_string=curr_string.lower()\n",
    "        word_tokens = word_tokenize(curr_string)\n",
    "        filtered_sentence = [] \n",
    "        for w in word_tokens: \n",
    "            if (w not in stop_words) and (w.isalpha()):\n",
    "                #word=lemmatizer.lemmatize(w)\n",
    "                filtered_sentence.append(w)\n",
    "        big_list[i]=' '.join(filtered_sentence)\n",
    "    return big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff2adb738807216feff73ae7a42e34b2512fe082"
   },
   "outputs": [],
   "source": [
    "train_des=data_clean(train_des)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8d72b641a8846b4d0d1077b20ac9647e4f9f1f18"
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'TEXT':train_des})\n",
    "df['word_count'] = df['TEXT'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df['word_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e51bb78adb0bdead1b4265dfef1124efbb990fc3"
   },
   "outputs": [],
   "source": [
    "df.to_csv('text_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "517f413e522f4fae0f5986003f9b1d9dae9c2bc6"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Input, Dropout, Activation\n",
    "from keras.layers import Conv2D,GlobalMaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Dense,merge,Lambda,Flatten\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "\n",
    "dim=300\n",
    "filter_size=32\n",
    "ksize=5\n",
    "\n",
    "#Model1\n",
    "inp1 = Input(shape=(None,dim,1))\n",
    "x1 = Conv2D(filter_size, kernel_size=(ksize,dim) ,padding='valid', kernel_initializer='normal')(inp1)\n",
    "x2 = GlobalMaxPooling2D()(x1)\n",
    "\n",
    "#Model2\n",
    "inp2 = Input(shape=(None,dim,1))\n",
    "y1 = Conv2D(filter_size, kernel_size=(ksize,dim) ,padding='valid', kernel_initializer='normal')(inp2)\n",
    "y2 = GlobalMaxPooling2D()(y1)\n",
    "\n",
    "#Model3\n",
    "merged = concatenate([x2,y2])\n",
    "out = Dense(10, activation=\"softmax\")(merged)\n",
    "model = Model(inputs=[inp1,inp2], outputs=out)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#Checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1d3881af8efdd72c9db2609237c4241298ea2ad9"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def data_list(big_list):\n",
    "    data_list=[]\n",
    "    for i in range(len(big_list)):\n",
    "        curr_string=big_list[i]\n",
    "        curr_string=curr_string.lower()\n",
    "        word_tokens = word_tokenize(curr_string)\n",
    "        data_list.append(word_tokens) \n",
    "    return data_list\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# train model\n",
    "new_list=data_list(train_des)\n",
    "model = Word2Vec(new_list,size=50, min_count=1)\n",
    "# summarize the loaded model\n",
    "print(model)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(len(words))\n",
    "# access vector for one word\n",
    "print(model['admission'])\n",
    "# save model\n",
    "model.save('model-mimic.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model-mimic.bin')\n",
    "#print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da0ec852b584574e2249c60dd12b7e35cf29515a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
